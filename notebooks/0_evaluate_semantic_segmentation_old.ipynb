{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pointtree.instance_segmentation import MultiStageAlgorithm\n",
    "from pointtree.evaluation import get_detections, get_instance_detection_metrics, get_instance_segmentation_metrics, get_semantic_segmentation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    'TreeML': {\n",
    "        '2023-01-09_5_1_37': {\n",
    "            'file_path': '<insert path>/Data/TreeML/2_semantic_segmentation/2023-01-09_5_1_37.csv',\n",
    "            'street': '2023-01-09\\_5\\_1\\_37',\n",
    "            'part': '',\n",
    "        },\n",
    "        '2023-01-09_17_2_18': {\n",
    "            'file_path': '<insert path>/Data/TreeML/2_semantic_segmentation/2023-01-09_17_2_18.csv',\n",
    "            'street': '2023-01-09\\_17\\_2\\_18',\n",
    "            'part': ''\n",
    "        },\n",
    "        '2023-01-10_7_6': {\n",
    "            'file_path': '<insert path>/Data/TreeML/2_semantic_segmentation/2023-01-10_7_6.csv',\n",
    "            'street': '2023-01-10\\_7\\_6',\n",
    "            'part': ''\n",
    "        },\n",
    "        '2023-01-12_35_34': {\n",
    "            'file_path': '<insert path>/Data/TreeML/2_semantic_segmentation/2023-01-12_35_34.csv',\n",
    "            'street': '2023-01-12\\_35\\_34',\n",
    "            'part': ''\n",
    "        },\n",
    "        '2023-01-12_65_64': {\n",
    "            'file_path': '<insert path>/Data/TreeML/2_semantic_segmentation/2023-01-12_65_64.csv',\n",
    "            'street': '2023-01-12\\_65\\_64',\n",
    "            'part': ''\n",
    "        },\n",
    "        '2023-01-16_44': {\n",
    "            'file_path': '<insert path>/Data/TreeML/2_semantic_segmentation/2023-01-16_44.csv',\n",
    "            'street': '2023-01-16\\_44',\n",
    "            'part': ''\n",
    "        }\n",
    "    },\n",
    "    'Essen': {\n",
    "        'altendorfer_part_1': {\n",
    "            'file_path': '<insert path>/Data/Essen/2_semantic_segmentation/Altendorfer_p1_min_1.csv',\n",
    "            'street': 'Altendorfer Straße',\n",
    "            'part': 'part 1'\n",
    "        },\n",
    "        'altendorfer_part_2': {\n",
    "            'file_path': '<insert path>/Data/Essen/2_semantic_segmentation/Altendorfer_p2_min_1.csv',\n",
    "            'street': 'Altendorfer Straße',\n",
    "            'part': 'part 2'\n",
    "        },\n",
    "        'altenessener_part_4': {\n",
    "            'file_path': '<insert path>/Data/Essen/2_semantic_segmentation/Essen3_p2_min_1.csv',\n",
    "            'street': 'Altenessener Straße',\n",
    "            'part': 'part 4'\n",
    "        },\n",
    "        'altenessener_part_5': {\n",
    "            'file_path': '<insert path>/Data/Essen/2_semantic_segmentation/Essen3_p3_min_1.csv',\n",
    "            'street': 'Altenessener Straße',\n",
    "            'part': 'part 5'\n",
    "        }\n",
    "    },\n",
    "    'Hamburg': {\n",
    "        'armgart_straße_part_1': {\n",
    "            'file_path': '<insert path>/Data/Hamburg/2_semantic_segmentation/000274_v2_min_1.csv',\n",
    "            'street': 'Armgartstraße',\n",
    "            'part': 'part 1'\n",
    "\n",
    "        },\n",
    "        'armgart_straße_part_2': {\n",
    "            'file_path': '<insert path>/Data/Hamburg/2_semantic_segmentation/000275_000276_min_1.csv',\n",
    "            'street': 'Armgartstraße',\n",
    "            'part': 'part 2'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "output_dir = '<insert path>/Data/3DGeoInfoExperiments/Watershed-Basic'\n",
    "min_iou_match = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for dataset in datasets:\n",
    "    os.makedirs(os.path.join(output_dir, dataset), exist_ok=True)\n",
    "    for file_id, file_infos in datasets[dataset].items():\n",
    "        for (experiment_label, semclass_id_column, specificclass_id_column) in [\n",
    "            ('gt', 'semclassid', 'specificclassid'),\n",
    "            ('dl', 'semclassidpredicted', 'specificclassidpredicted')\n",
    "        ]:\n",
    "            if experiment_label == \"segmentation_ground_truth\" and dataset == \"TreeML\":\n",
    "                continue\n",
    "            point_cloud = pd.read_csv(file_infos['file_path'])\n",
    "\n",
    "            point_cloud = point_cloud.rename({'//X': 'X'}, axis=1)\n",
    "            point_cloud = point_cloud.rename(str.lower, axis=1)\n",
    "\n",
    "            columns_to_keep = ['x', 'y', 'z', 'incomplete', 'low_density', 'instance_id', 'semclassidpredicted', 'specificclassidpredicted']\n",
    "\n",
    "            if 'classification' in point_cloud.columns:\n",
    "                columns_to_keep.append('classification')\n",
    "            if 'semclassid' in point_cloud.columns:\n",
    "                columns_to_keep.append('semclassid')\n",
    "            if 'specificclassid' in point_cloud.columns:\n",
    "                columns_to_keep.append('specificclassid')\n",
    "\n",
    "            point_cloud = point_cloud[columns_to_keep]\n",
    "\n",
    "            class_mapping = {\n",
    "                (0, 0): 0,\n",
    "                (1, 0): 0,\n",
    "                (1, 1): 1,\n",
    "                (1, 2): 2,\n",
    "                (1, 3): 3,\n",
    "                (2, 0): 4,\n",
    "                (3, 0): 4,\n",
    "                (4, 0): 4,\n",
    "                (4, 1): 4\n",
    "            }\n",
    "                \n",
    "            print(\"Process\", file_id, experiment_label)\n",
    "\n",
    "            instance_seg_folder = os.path.join(output_dir, 'Data', dataset, '3_instance_segmentation')\n",
    "            os.makedirs(instance_seg_folder, exist_ok=True)\n",
    "            metrics_folder = os.path.join(output_dir, 'Metrics', dataset, file_id)\n",
    "            os.makedirs(metrics_folder, exist_ok=True)\n",
    "            visualization_folder = os.path.join(output_dir, 'Data', dataset, 'visualizations', file_id, experiment_label)\n",
    "            os.makedirs(visualization_folder, exist_ok=True)\n",
    "\n",
    "            if experiment_label == \"dl\":\n",
    "                if dataset != \"TreeML\":\n",
    "                    class_map = {\n",
    "                        'other': 0,\n",
    "                        'trunk': 1,\n",
    "                        'crown': 2,\n",
    "                        'branch': 3,\n",
    "                        'low_vegetation': 4\n",
    "                    }\n",
    "                    point_cloud[\"classification_target\"] = np.vectorize(lambda x, y: class_mapping[(x, y)])(\n",
    "                        point_cloud[\"semclassid\"].to_numpy(), point_cloud[\"specificclassid\"].to_numpy()\n",
    "                    )\n",
    "                    point_cloud[\"classification_prediction\"] = np.vectorize(lambda x, y: class_mapping[(x, y)])(\n",
    "                        point_cloud[\"semclassidpredicted\"].to_numpy(), point_cloud[\"specificclassidpredicted\"].to_numpy()\n",
    "                    )\n",
    "\n",
    "                else:\n",
    "                    class_map = {\n",
    "                        'other': 0,\n",
    "                        'building': 1,\n",
    "                        'tree': 2\n",
    "                    }\n",
    "                    target_class_mapping = {\n",
    "                        0: 0,\n",
    "                        1: 0,\n",
    "                        2: 1,\n",
    "                    }\n",
    "                    prediction_class_mapping = {\n",
    "                        (0, 0): 0,\n",
    "                        (1, 0): 0,\n",
    "                        (1, 1): 1,\n",
    "                        (1, 2): 1,\n",
    "                        (1, 3): 1,\n",
    "                        (2, 0): 0\n",
    "                    }\n",
    "                    point_cloud[\"classification_target\"] = np.vectorize(lambda x: target_class_mapping[x])(\n",
    "                        point_cloud[\"classification\"].to_numpy()\n",
    "                    )\n",
    "                    point_cloud[\"classification_prediction\"] = np.vectorize(lambda x, y: prediction_class_mapping[(x, y)])(\n",
    "                        point_cloud[\"semclassidpredicted\"].to_numpy(), point_cloud[\"specificclassidpredicted\"].to_numpy()\n",
    "                    )\n",
    "                semantic_segmentation_metrics = get_semantic_segmentation_metrics(point_cloud['classification_target'].to_numpy(),\n",
    "                                                                                point_cloud[\"classification_prediction\"].to_numpy(),\n",
    "                                                                                class_map)\n",
    "                semantic_segmentation_metrics[\"Dataset\"] = dataset\n",
    "                semantic_segmentation_metrics[\"FileID\"] = file_id\n",
    "                semantic_segmentation_metrics[\"Street\"] = file_infos['street']\n",
    "                semantic_segmentation_metrics[\"Part\"] = file_infos['part']\n",
    "                semantic_segmentation_metrics.to_csv(os.path.join(metrics_folder, f'{file_id}_semantic_segmentation.csv'), index=False)\n",
    "\n",
    "            point_cloud[\"classification\"] = np.vectorize(lambda x, y: class_mapping[(x, y)])(\n",
    "                point_cloud[semclass_id_column].to_numpy(), point_cloud[specificclass_id_column].to_numpy()\n",
    "            )\n",
    "\n",
    "            point_cloud = point_cloud.drop([\"classification_target\", \"classification_prediction\", \"semclassid\", \"specificclassid\", \"semclassidpredicted\", \"specificclassidpredicted\"],\n",
    "                                axis=1, errors='ignore')\n",
    "\n",
    "            segmentation_algorithm = MultiStageSegmentationAlgorithm(trunk_class_id=1, crown_class_id=2,\n",
    "                                                                     branch_class_id=3,\n",
    "                                                                     algorithm=\"watershed_tree_top_positions\",\n",
    "                                                                     correct_watershed=False,\n",
    "                                                                     grid_size=0.03,\n",
    "                                                                     visualization_folder=visualization_folder)\n",
    "            point_cloud[\"instance_id_predicted\"] = segmentation_algorithm.segment(point_cloud,\n",
    "                                                                                    f'{dataset}_{file_id}_{experiment_label}')\n",
    "            \n",
    "            point_cloud.to_csv(os.path.join(instance_seg_folder, f'{file_id}_{experiment_label}.csv'), index=False)\n",
    "\n",
    "            runtime_stats = segmentation_algorithm.runtime_stats()\n",
    "            runtime_stats[\"Dataset\"] = dataset\n",
    "            runtime_stats[\"FileID\"] = file_id\n",
    "            runtime_stats[\"Street\"] = file_infos['street']\n",
    "            runtime_stats[\"Part\"] = file_infos['part']\n",
    "            runtime_stats[\"SemanticSegmentation\"] = experiment_label\n",
    "            runtime_stats.to_csv(os.path.join(metrics_folder, f'{file_id}_{experiment_label}_runtime.csv'), index=False)\n",
    "\n",
    "            matched_gts, matched_preds, iou_matrix = get_detections(\n",
    "                point_cloud[\"instance_id\"].to_numpy(),\n",
    "                point_cloud[\"instance_id_predicted\"].to_numpy(),\n",
    "                min_iou_match=min_iou_match\n",
    "            )\n",
    "\n",
    "            instance_detection_metrics = get_instance_detection_metrics(point_cloud[\"instance_id\"].to_numpy(), \n",
    "                                                                    point_cloud[\"instance_id_predicted\"].to_numpy(),\n",
    "                                                                    matched_gts,\n",
    "                                                                    matched_preds)\n",
    "            instance_segmentation_metrics = get_instance_segmentation_metrics(point_cloud[\"instance_id\"].to_numpy(), \n",
    "                                                                    point_cloud[\"instance_id_predicted\"].to_numpy(),\n",
    "                                                                    matched_gts,\n",
    "                                                                    matched_preds)\n",
    "\n",
    "            instance_detection_metrics[\"Dataset\"] = dataset\n",
    "            instance_detection_metrics[\"FileID\"] = file_id\n",
    "            instance_detection_metrics[\"Street\"] = file_infos['street']\n",
    "            instance_detection_metrics[\"Part\"] = file_infos['part']\n",
    "            instance_detection_metrics[\"SemanticSegmentation\"] = experiment_label\n",
    "            instance_detection_metrics = instance_detection_metrics[['Dataset', 'FileID', 'Street', 'Part', 'SemanticSegmentation', 'TP', 'FP', 'FN', 'Precision', 'Recall']]\n",
    "            instance_detection_metrics.to_csv(os.path.join(metrics_folder, f'{file_id}_{experiment_label}_instance_detection.csv'),\n",
    "                                              index=False)\n",
    "\n",
    "            # instance_segmentation_metrics[\"mIoU\"] = instance_segmentation_metrics[\"iou\"].mean() if len(instance_segmentation_metrics) > 0 else np.nan\n",
    "            # instance_segmentation_metrics[\"mPrecision\"] = instance_segmentation_metrics[\"precision\"].mean() if len(instance_segmentation_metrics) > 0 else np.nan\n",
    "            # instance_segmentation_metrics[\"mRecall\"] = instance_segmentation_metrics[\"recall\"].mean() if len(instance_segmentation_metrics) > 0 else np.nan\n",
    "            instance_segmentation_metrics[\"Dataset\"] = dataset\n",
    "            instance_segmentation_metrics[\"FileID\"] = file_id\n",
    "            instance_segmentation_metrics[\"Street\"] = file_infos['street']\n",
    "            instance_segmentation_metrics[\"Part\"] = file_infos['part']\n",
    "            instance_segmentation_metrics[\"SemanticSegmentation\"] = experiment_label\n",
    "            instance_segmentation_metrics = instance_segmentation_metrics[['Dataset', 'FileID', 'Street', 'Part', 'SemanticSegmentation', 'IdTarget', 'IdPred', 'IoU', 'Precision', 'Recall']]\n",
    "            instance_segmentation_metrics.to_csv(os.path.join(metrics_folder, f'{file_id}_{experiment_label}_instance_segmentation.csv'),\n",
    "                                                 index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    metrics_folder = os.path.join(output_dir, 'Metrics', dataset)\n",
    "\n",
    "    for (experiment_label, semclass_id_column, specificclass_id_column) in [\n",
    "            ('gt', 'semclassid', 'specificclassid'),\n",
    "            ('dl', 'semclassidpredicted', 'specificclassidpredicted')\n",
    "        ]:\n",
    "        runtime_results = []\n",
    "        semantic_segmentation_results = []\n",
    "        detection_results = []\n",
    "        segmentation_results = []\n",
    "        for file_id, file_path in datasets[dataset].items():\n",
    "                print(\"file_id\", file_id)\n",
    "                runtime_metrics_file = os.path.join(metrics_folder, file_id,\n",
    "                                                                f'{file_id}_{experiment_label}_runtime.csv')\n",
    "                if os.path.exists(runtime_metrics_file):\n",
    "                    runtime_results.append(pd.read_csv(runtime_metrics_file))\n",
    "                if experiment_label == \"dl\":\n",
    "                    semantic_segmentation_metrics_file = os.path.join(metrics_folder, file_id,\n",
    "                                                                    f'{file_id}_semantic_segmentation.csv')\n",
    "                    if os.path.exists(semantic_segmentation_metrics_file):\n",
    "                        semantic_segmentation_results.append(pd.read_csv(semantic_segmentation_metrics_file))\n",
    "\n",
    "                detection_metrics_file = os.path.join(metrics_folder, file_id, f'{file_id}_{experiment_label}_instance_detection.csv')\n",
    "                if os.path.exists(detection_metrics_file):\n",
    "                    detection_results.append(pd.read_csv(detection_metrics_file))\n",
    "\n",
    "                segmentation_metrics_file = os.path.join(metrics_folder, file_id,\n",
    "                                                                f'{file_id}_{experiment_label}_instance_segmentation.csv')\n",
    "\n",
    "                if os.path.exists(segmentation_metrics_file):\n",
    "                    segmentation_results.append(pd.read_csv(segmentation_metrics_file))\n",
    "\n",
    "        if len(runtime_results) > 0:\n",
    "            runtime_results = pd.concat(runtime_results)\n",
    "            runtime_results.to_csv(os.path.join(metrics_folder, f'runtime_{dataset}.csv'), index=False)\n",
    "\n",
    "        if experiment_label == \"dl\" and len(semantic_segmentation_results) > 0:\n",
    "            semantic_segmentation_results = pd.concat(semantic_segmentation_results)\n",
    "            semantic_segmentation_results.to_csv(os.path.join(metrics_folder, f'semantic_segmentation_metrics_{dataset}.csv'), index=False)\n",
    "\n",
    "        if len(segmentation_results) > 0:\n",
    "            segmentation_results = pd.concat(segmentation_results)\n",
    "            segmentation_results_aggregated = segmentation_results[['Dataset', 'FileID', 'Street', 'Part', 'SemanticSegmentation', 'IoU', 'Precision', 'Recall']].groupby(['Dataset', 'FileID', 'Street', 'Part', 'SemanticSegmentation']).mean()\n",
    "            segmentation_results_aggregated[segmentation_results_aggregated.index.names] = segmentation_results_aggregated.index.to_list()\n",
    "            segmentation_results_aggregated.index = np.arange(len(segmentation_results_aggregated), dtype=np.int64)\n",
    "            segmentation_results_aggregated = pd.DataFrame(segmentation_results_aggregated)\n",
    "            segmentation_results_aggregated = segmentation_results_aggregated.rename({'IoU': 'mIoU', 'Precision': 'mPrecision', 'Recall': 'mRecall'}, axis=1)\n",
    "            segmentation_results_total = {\n",
    "                \"Dataset\": dataset,\n",
    "                \"FileID\": \"total\",\n",
    "                \"Street\": \"Total\",\n",
    "                \"Part\": \"\",\n",
    "                \"SemanticSegmentation\": experiment_label,\n",
    "                \"mIoU\": segmentation_results[\"IoU\"].mean() if len(segmentation_results) > 0 else np.nan,\n",
    "                \"mPrecision\": segmentation_results[\"Precision\"].mean() if len(segmentation_results) > 0 else np.nan,\n",
    "                \"mRecall\": segmentation_results[\"Recall\"].mean() if len(segmentation_results) > 0 else np.nan\n",
    "            }\n",
    "            segmentation_results = pd.concat([segmentation_results_aggregated, pd.DataFrame([segmentation_results_total])])\n",
    "            segmentation_results = segmentation_results[['Dataset', 'FileID', 'Street', 'Part', 'SemanticSegmentation', 'mIoU', 'mPrecision', 'mRecall']]\n",
    "            segmentation_results.to_csv(os.path.join(metrics_folder, f'instance_segmentation_metrics_{dataset}_{experiment_label}.csv'), index=False)\n",
    "\n",
    "        if len(detection_results) > 0:\n",
    "            detection_results = pd.concat(detection_results)\n",
    "            detection_results_total = {\n",
    "                \"Dataset\": dataset,\n",
    "                \"FileID\": \"total\",\n",
    "                \"Street\": \"Total\",\n",
    "                \"Part\": \"\",\n",
    "                \"SemanticSegmentation\": experiment_label,\n",
    "                \"TP\": detection_results[\"TP\"].sum(),\n",
    "                \"FP\": detection_results[\"FP\"].sum(),\n",
    "                \"FN\": detection_results[\"FN\"].sum(),\n",
    "                \"Precision\": detection_results[\"TP\"].sum() / (detection_results[\"TP\"].sum() + detection_results[\"FP\"].sum()),\n",
    "                \"Recall\": detection_results[\"TP\"].sum() / (detection_results[\"TP\"].sum() + detection_results[\"FN\"].sum()),\n",
    "                # \"mIoU\": segmentation_results[\"iou\"].mean() if len(segmentation_results) > 0 else np.nan,\n",
    "                # \"mPrecision\": segmentation_results[\"precision\"].mean() if len(segmentation_results) > 0 else np.nan,\n",
    "                # \"mRecall\": segmentation_results[\"recall\"].mean() if len(segmentation_results) > 0 else np.nan\n",
    "            }\n",
    "            detection_results = pd.concat([detection_results, pd.DataFrame([detection_results_total])])\n",
    "            detection_results['Precision'] = np.round(detection_results['Precision'].to_numpy(), 2)\n",
    "            detection_results['Recall'] = np.round(detection_results['Recall'].to_numpy(), 2)\n",
    "            detection_results = detection_results[['Dataset', 'FileID', 'Street', 'Part', 'SemanticSegmentation', 'TP', 'FP', 'FN', 'Precision', 'Recall']]\n",
    "            detection_results.to_csv(os.path.join(metrics_folder, f'instance_detection_metrics_{dataset}_{experiment_label}.csv'), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_results_aggregated[segmentation_results_aggregated.index.names] = segmentation_results_aggregated.index.to_list()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
