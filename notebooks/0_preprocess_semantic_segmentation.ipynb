{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pointtree.io import PointCloudIoData, PointCloudWriter\n",
    "from pointtree.instance_segmentation import remap_instance_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '<insert path>'\n",
    "\n",
    "datasets = {\n",
    "    'TreeML': {\n",
    "        '2023-01-09_5_1_37': {\n",
    "            'file_path': '2023-01-09_5_1_37.csv',\n",
    "            'street': '2023-01-09\\_5\\_1\\_37',\n",
    "            'part': '',\n",
    "        },\n",
    "        '2023-01-09_17_2_18': {\n",
    "            'file_path': '2023-01-09_17_2_18.csv',\n",
    "            'street': '2023-01-09\\_17\\_2\\_18',\n",
    "            'part': ''\n",
    "        },\n",
    "        '2023-01-10_7_6': {\n",
    "            'file_path': '2023-01-10_7_6.csv',\n",
    "            'street': '2023-01-10\\_7\\_6',\n",
    "            'part': ''\n",
    "        },\n",
    "        '2023-01-12_35_34': {\n",
    "            'file_path': '2023-01-12_35_34.csv',\n",
    "            'street': '2023-01-12\\_35\\_34',\n",
    "            'part': ''\n",
    "        },\n",
    "        '2023-01-12_65_64': {\n",
    "            'file_path': '2023-01-12_65_64.csv',\n",
    "            'street': '2023-01-12\\_65\\_64',\n",
    "            'part': ''\n",
    "        },\n",
    "        '2023-01-16_44': {\n",
    "            'file_path': '2023-01-16_44.csv',\n",
    "            'street': '2023-01-16\\_44',\n",
    "            'part': ''\n",
    "        },\n",
    "        '2023-01-09_tum_campus': {\n",
    "            'file_path': '2023-01-09_tum_campus.csv',\n",
    "            'street': '2023-01-09\\_tum\\_campus',\n",
    "            'part': ''\n",
    "        },\n",
    "        '2023-01-13_42': {\n",
    "            'file_path': '2023-01-13_42.csv',\n",
    "            'street': '2023-01-13\\_42',\n",
    "            'part': ''\n",
    "        },\n",
    "        '2023-01-13_61': {\n",
    "            'file_path': '2023-01-13_61.csv',\n",
    "            'street': '2023-01-13\\_61',\n",
    "            'part': ''\n",
    "        },\n",
    "        '2023-01-12_48': {\n",
    "            'file_path': '2023-01-12_48.csv',\n",
    "            'street': '2023-01-12\\_48',\n",
    "            'part': ''\n",
    "        },\n",
    "        '2023-01-12_58': {\n",
    "            'file_path': '2023-01-12_58.csv',\n",
    "            'street': '2023-01-12\\_58',\n",
    "            'part': ''\n",
    "        },\n",
    "        '2023-01-13_74': {\n",
    "            'file_path': '2023-01-13_74.csv',\n",
    "            'street': '2023-01-13\\_74',\n",
    "            'part': ''\n",
    "        },\n",
    "        '2023-01-13_4': {\n",
    "            'file_path': '2023-01-13_4.csv',\n",
    "            'street': '2023-01-13\\_4',\n",
    "            'part': ''\n",
    "        },\n",
    "        '2023-01-13_52': {\n",
    "            'file_path': '2023-01-13_52.csv',\n",
    "            'street': '2023-01-13\\_52',\n",
    "            'part': ''\n",
    "        },\n",
    "        '2023-01-12_28': {\n",
    "            'file_path': '2023-01-12_28.csv',\n",
    "            'street': '2023-01-12\\_28',\n",
    "            'part': ''\n",
    "        },\n",
    "        '2023-01-16_22': {\n",
    "            'file_path': '2023-01-16_22.csv',\n",
    "            'street': '2023-01-16\\_22',\n",
    "            'part': ''\n",
    "        },\n",
    "        '2023-01-12_57': {\n",
    "            'file_path': '2023-01-12_57.csv',\n",
    "            'street': '2023-01-12\\_57',\n",
    "            'part': ''\n",
    "        },\n",
    "        '2023-01-16_43': {\n",
    "            'file_path': '2023-01-16_43.csv',\n",
    "            'street': '2023-01-16\\_43',\n",
    "            'part': ''\n",
    "        },\n",
    "        '2023-01-16_12': {\n",
    "            'file_path': '2023-01-16_12.csv',\n",
    "            'street': '2023-01-16\\_12',\n",
    "            'part': ''\n",
    "        },\n",
    "        '2023-01-12_56': {\n",
    "            'file_path': '2023-01-12_56.csv',\n",
    "            'street': '2023-01-12\\_56',\n",
    "            'part': ''\n",
    "        },\n",
    "    },\n",
    "    'Essen': {\n",
    "        'altendorfer_part_1': {\n",
    "            'file_path': 'Altendorfer_p1_min_1.csv',\n",
    "            'street': 'Altendorfer Straße',\n",
    "            'part': 'part 1',\n",
    "        },\n",
    "        'altendorfer_part_2': {\n",
    "            'file_path': 'Altendorfer_p2_min_1.csv',\n",
    "            'street': 'Altendorfer Straße',\n",
    "            'part': 'part 2'\n",
    "        },\n",
    "        'altenessener_part_4': {\n",
    "            'file_path': 'Essen3_p2_min_1.csv',\n",
    "            'street': 'Altenessener Straße',\n",
    "            'part': 'part 4'\n",
    "        },\n",
    "        'altenessener_part_5': {\n",
    "            'file_path': 'Essen3_p3_min_1.csv',\n",
    "            'street': 'Altenessener Straße',\n",
    "            'part': 'part 5'\n",
    "        }\n",
    "    },\n",
    "    'Hamburg': {\n",
    "        'armgart_straße_part_1': {\n",
    "            'file_path': '000274_v2_min_1.csv',\n",
    "            'street': 'Armgartstraße',\n",
    "            'part': 'part 1'\n",
    "        },\n",
    "        'armgart_straße_part_2': {\n",
    "            'file_path': '000275_000276_min_1.csv',\n",
    "            'street': 'Armgartstraße',\n",
    "            'part': 'part 2'\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = PointCloudWriter()\n",
    "\n",
    "for dataset in datasets:\n",
    "    os.makedirs(os.path.join(base_dir, 'Data', dataset), exist_ok=True)\n",
    "    for file_id, file_infos in datasets[dataset].items():\n",
    "        print(\"Process\", file_id)\n",
    "\n",
    "        point_cloud = pd.read_csv(os.path.join(base_dir, 'Data', dataset, '2_semantic_segmentation', file_infos['file_path']))\n",
    "        point_cloud = point_cloud.rename({'//X': 'X'}, axis=1)\n",
    "        point_cloud = point_cloud.rename(str.lower, axis=1)\n",
    "\n",
    "        columns_to_keep = ['x', 'y', 'z', 'instance_id', 'semclassidpredicted', 'specificclassidpredicted']\n",
    "\n",
    "        if 'classification' in point_cloud.columns:\n",
    "            columns_to_keep.append('classification')\n",
    "        if 'semclassid' in point_cloud.columns:\n",
    "            columns_to_keep.append('semclassid')\n",
    "        if 'specificclassid' in point_cloud.columns:\n",
    "            columns_to_keep.append('specificclassid')\n",
    "\n",
    "        point_cloud = point_cloud[columns_to_keep]\n",
    "\n",
    "        class_mapping = {\n",
    "            (0, 0): 0,\n",
    "            (1, 0): 0,\n",
    "            (1, 1): 1,\n",
    "            (1, 2): 2,\n",
    "            (1, 3): 3,\n",
    "            (2, 0): 4,\n",
    "            (3, 0): 4,\n",
    "            (4, 0): 4,\n",
    "            (4, 1): 4\n",
    "        }\n",
    "\n",
    "        if dataset == \"TreeML\":\n",
    "            target_class_mapping = {\n",
    "                0: 0,\n",
    "                1: 0,\n",
    "                2: 2,\n",
    "            }\n",
    "            point_cloud[\"classification_target\"] = np.vectorize(lambda x: target_class_mapping[x])(\n",
    "                point_cloud[\"classification\"].to_numpy()\n",
    "            )\n",
    "        else:\n",
    "            point_cloud[\"classification_target\"] = np.vectorize(lambda x, y: class_mapping[(x, y)])(\n",
    "                point_cloud[\"semclassid\"].to_numpy(), point_cloud[\"specificclassid\"].to_numpy()\n",
    "            )\n",
    "        point_cloud[\"classification_prediction\"] = np.vectorize(lambda x, y: class_mapping[(x, y)])(\n",
    "            point_cloud[\"semclassidpredicted\"].to_numpy(), point_cloud[\"specificclassidpredicted\"].to_numpy()\n",
    "        )\n",
    "        point_cloud.loc[~(point_cloud[\"classification_target\"].isin([1, 2, 3])), \"instance_id\"] = -1\n",
    "        point_cloud[\"instance_id\"] = remap_instance_ids(point_cloud[\"instance_id\"].to_numpy())[0]\n",
    "\n",
    "        data_dir = os.path.join(base_dir, 'Data', dataset, '3_semantic_segmentation_processed')\n",
    "        os.makedirs(data_dir, exist_ok=True)\n",
    "        columns_to_keep = [\"x\", \"y\", \"z\", \"instance_id\", \"classification_target\", \"classification_prediction\"]\n",
    "        if \"intensity\" in point_cloud.columns:\n",
    "            columns_to_keep.append(\"intensity\")\n",
    "        point_cloud_io_data = PointCloudIoData(point_cloud, x_max_resolution=1e-6, y_max_resolution=1e-6, z_max_resolution=1e-6)\n",
    "        writer.write(point_cloud_io_data, os.path.join(data_dir, file_infos['file_path'].replace('.csv', '.laz')), columns=columns_to_keep)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
