{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Semantic Segmentation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pointtree.evaluation import semantic_segmentation_metrics\n",
    "from pointtree.io import PointCloudReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Point cloud files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '<insert path>'\n",
    "\n",
    "datasets = {\n",
    "    'TreeML': {\n",
    "        '2023-01-09_tum_campus': {\n",
    "            'file_path': '2023-01-09_tum_campus.laz',\n",
    "            'street': '2023-01-09\\_tum\\_campus',\n",
    "            'part': '',\n",
    "            'subset': 'test',\n",
    "            'short': 'TTC'\n",
    "        },\n",
    "        '2023-01-12_57': {\n",
    "            'file_path': '2023-01-12_57.laz',\n",
    "            'street': '2023-01-12\\_57',\n",
    "            'part': '',\n",
    "            'subset': 'test',\n",
    "            'short': 'T12\\_57'\n",
    "        },\n",
    "        '2023-01-12_58': {\n",
    "            'file_path': '2023-01-12_58.laz',\n",
    "            'street': '2023-01-12\\_58',\n",
    "            'part': '',\n",
    "            'subset': 'test',\n",
    "            'short': 'T12\\_58'\n",
    "        },\n",
    "        '2023-01-16_12': {\n",
    "            'file_path': '2023-01-16_12.laz',\n",
    "            'street': '2023-01-16\\_12',\n",
    "            'part': '',\n",
    "            'subset': 'test',\n",
    "            'short': 'T16\\_12'\n",
    "        },\n",
    "        '2023-01-16_43': {\n",
    "            'file_path': '2023-01-16_43.laz',\n",
    "            'street': '2023-01-16\\_43',\n",
    "            'part': '',\n",
    "            'subset': 'test',\n",
    "            'short': 'T16\\_43'\n",
    "        },\n",
    "    },\n",
    "    'Essen': {\n",
    "        'altendorfer_part_1': {\n",
    "            'file_path': 'Altendorfer_p1_min_1.laz',\n",
    "            'street': 'Altendorfer Straße',\n",
    "            'part': 'part 1',\n",
    "            'subset': 'val',\n",
    "            'short': 'EAD1'\n",
    "        },\n",
    "        'altendorfer_part_2': {\n",
    "            'file_path': 'Altendorfer_p2_min_1.laz',\n",
    "            'street': 'Altendorfer Straße',\n",
    "            'part': 'part 2',\n",
    "            'subset': 'val',\n",
    "            'short': 'EAD2'\n",
    "        },\n",
    "        'altenessener_part_4': {\n",
    "            'file_path': 'Essen3_p2_min_1.laz',\n",
    "            'street': 'Altenessener Straße',\n",
    "            'part': 'part 4',\n",
    "            'subset': 'val',\n",
    "            'short': 'EAE4'\n",
    "        },\n",
    "        'altenessener_part_5': {\n",
    "            'file_path': 'Essen3_p3_min_1.laz',\n",
    "            'street': 'Altenessener Straße',\n",
    "            'part': 'part 5',\n",
    "            'subset': 'test',\n",
    "            'short': 'EAE5'\n",
    "        }\n",
    "    },\n",
    "    'Hamburg': {\n",
    "        'armgart_straße_part_1': {\n",
    "            'file_path': '000274_v2_min_1.laz',\n",
    "            'street': 'Armgartstraße',\n",
    "            'part': 'part 1',\n",
    "            'subset': 'val',\n",
    "            'short': 'HAG1'\n",
    "        },\n",
    "        'armgart_straße_part_2': {\n",
    "            'file_path': '000275_000276_min_1.laz',\n",
    "            'street': 'Armgartstraße',\n",
    "            'part': 'part 2',\n",
    "            'subset': 'test',\n",
    "            'short': 'HAG2'\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate semantic segmentation metrics for each point cloud file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PointCloudReader()\n",
    "\n",
    "for dataset in datasets:\n",
    "    for file_id, file_infos in datasets[dataset].items():\n",
    "        print(\"Process\", file_id)\n",
    "        metrics_folder = os.path.join(base_dir, 'Metrics', dataset, file_id)\n",
    "        os.makedirs(metrics_folder, exist_ok=True)\n",
    "\n",
    "        point_cloud = reader.read(os.path.join(base_dir, 'Data', dataset, '3_semantic_segmentation_processed',\n",
    "                                               file_infos['file_path'])).data\n",
    "    \n",
    "        class_map = {\n",
    "            \"other\": 0,\n",
    "            \"tree_trunk\": 1,\n",
    "            \"tree_crown\": 2,\n",
    "            \"tree_branch\": 3,\n",
    "            \"low_vegetation\": 4\n",
    "        }\n",
    "\n",
    "        aggregate_classes = {\"tree\": [1, 2, 3]}\n",
    "        if dataset == \"TreeML\":\n",
    "            aggregate_classes[\"other\"] = [0, 4]\n",
    "\n",
    "        metrics = semantic_segmentation_metrics(point_cloud['classification_target'].to_numpy(),\n",
    "                                                point_cloud[\"classification_prediction\"].to_numpy(),\n",
    "                                                class_map, aggregate_classes=aggregate_classes)\n",
    "        metrics[\"Dataset\"] = dataset\n",
    "        metrics[\"FileID\"] = file_id\n",
    "        metrics[\"Street\"] = file_infos['street']\n",
    "        metrics[\"Part\"] = file_infos['part']\n",
    "        metrics[\"Subset\"] = file_infos['subset']\n",
    "        metrics[\"Short\"] = file_infos['short']\n",
    "        metrics[\"Points\"] = len(point_cloud)\n",
    "        metrics[\"TreePoints\"] = len(point_cloud[point_cloud[\"classification_target\"].isin([1, 2, 3])])\n",
    "        instance_ids = point_cloud[np.logical_and(point_cloud[\"instance_id\"] != -1,\n",
    "                                                 point_cloud[\"classification_target\"].isin([1, 2, 3]))][\"instance_id\"]\n",
    "        metrics[\"Trees\"] = len(instance_ids.unique())\n",
    "\n",
    "        complete_trees = 0\n",
    "        for instance_id in instance_ids.unique():\n",
    "            instance_trunk_points = point_cloud[np.logical_and(point_cloud[\"classification_target\"] == 1,\n",
    "                                                               point_cloud[\"instance_id\"] == instance_id)]\n",
    "            instance_crown_points = point_cloud[np.logical_and(point_cloud[\"classification_target\"] == 2,\n",
    "                                                               point_cloud[\"instance_id\"] == instance_id)]\n",
    "            if len(instance_trunk_points) > 0 and len(instance_crown_points) > 0:\n",
    "                complete_trees += 1\n",
    "\n",
    "        metrics[\"CompleteTrees\"] = complete_trees\n",
    "\n",
    "        metrics = pd.DataFrame([metrics])\n",
    "    \n",
    "        file_name = \".\".join(file_infos['file_path'].split(\".\")[:-1]) + \"_semantic_segmentation.csv\"\n",
    "        metrics.to_csv(os.path.join(metrics_folder, file_name), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Aggregate metrics for all point cloud files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_decimals = 3\n",
    "\n",
    "metrics = []\n",
    "for dataset in datasets:\n",
    "    for file_id, file_infos in datasets[dataset].items():\n",
    "        metrics_folder = os.path.join(base_dir, 'Metrics', dataset, file_id)\n",
    "        file_name = \".\".join(file_infos['file_path'].split(\".\")[:-1]) + \"_semantic_segmentation.csv\"\n",
    "        semantic_segmentation_metrics_file = os.path.join(metrics_folder, file_name)\n",
    "        if os.path.exists(semantic_segmentation_metrics_file):\n",
    "            metrics.append(pd.read_csv(semantic_segmentation_metrics_file))\n",
    "\n",
    "metrics = pd.concat(metrics)\n",
    "metrics = metrics.rename({\"m_iou\": \"mIoU\",\n",
    "                          \"m_iou_aggregated\": \"mIoUAggr\",\n",
    "                          \"tree_iou\": \"TreeIoU\",\n",
    "                          \"tree_trunk_iou\": \"TrunkIoU\",\n",
    "                          \"tree_branch_iou\": \"BranchIoU\",\n",
    "                          \"tree_crown_iou\": \"CrownIoU\",\n",
    "                          \"low_vegetation_iou\": \"LowVegetationIoU\",\n",
    "                          \"other_iou\": \"OtherIoU\"}, axis=1)\n",
    "metric_columns = [\"mIoU\", \"mIoUAggr\", \"TreeIoU\", \"TrunkIoU\", \"BranchIoU\", \"CrownIoU\", \"LowVegetationIoU\", \"OtherIoU\"]\n",
    "for metric_column in metric_columns:\n",
    "    metrics[metric_column] = np.round(metrics[metric_column].to_numpy(), output_decimals)\n",
    "metrics = metrics[[\"Dataset\", \"Street\", \"Part\", \"Subset\", \"Short\", \"Points\", \"TreePoints\", \"Trees\", \"CompleteTrees\", *metric_columns]]\n",
    "metrics.to_csv(os.path.join(base_dir, 'Metrics', f'dataset_semantic_segmentation_metrics.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
