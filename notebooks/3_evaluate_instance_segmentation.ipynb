{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Tree Instance Segmentation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pointtree.instance_segmentation import MultiStageAlgorithm\n",
    "from pointtree.evaluation import match_instances, instance_segmentation_metrics\n",
    "from pointtorch import PointCloud, read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Point cloud files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '<insert path>'\n",
    "\n",
    "datasets = {\n",
    "    'TreeML': {\n",
    "        # '2023-01-09_tum_campus': {\n",
    "        #     'file_path': '2023-01-09_tum_campus_demo.laz',\n",
    "        #     'street': '2023-01-09\\_tum\\_campus',\n",
    "        #     'part': '',\n",
    "        #     'subset': 'test',\n",
    "        #     'short': 'TTC'\n",
    "        # },\n",
    "        '2023-01-09_tum_campus': {\n",
    "            'file_path': '2023-01-09_tum_campus.laz',\n",
    "            'street': '2023-01-09\\_tum\\_campus',\n",
    "            'part': '',\n",
    "            'subset': 'test',\n",
    "            'short': 'TTC'\n",
    "        },\n",
    "        '2023-01-12_57': {\n",
    "            'file_path': '2023-01-12_57.laz',\n",
    "            'street': '2023-01-12\\_57',\n",
    "            'part': '',\n",
    "            'subset': 'test',\n",
    "            'short': 'T12\\_57'\n",
    "        },\n",
    "        '2023-01-12_58': {\n",
    "            'file_path': '2023-01-12_58.laz',\n",
    "            'street': '2023-01-12\\_58',\n",
    "            'part': '',\n",
    "            'subset': 'test',\n",
    "            'short': 'T12\\_58'\n",
    "        },\n",
    "        '2023-01-16_12': {\n",
    "            'file_path': '2023-01-16_12.laz',\n",
    "            'street': '2023-01-16\\_12',\n",
    "            'part': '',\n",
    "            'subset': 'test',\n",
    "            'short': 'T16\\_12'\n",
    "        },\n",
    "        '2023-01-16_43': {\n",
    "            'file_path': '2023-01-16_43.laz',\n",
    "            'street': '2023-01-16\\_43',\n",
    "            'part': '',\n",
    "            'subset': 'test',\n",
    "            'short': 'T16\\_43'\n",
    "        },\n",
    "    },\n",
    "    'Essen': {\n",
    "        'altendorfer_part_1': {\n",
    "            'file_path': 'Altendorfer_p1_min_1.laz',\n",
    "            'street': 'Altendorfer Straße',\n",
    "            'part': 'part 1',\n",
    "            'subset': 'val',\n",
    "            'short': 'EAD1'\n",
    "        },\n",
    "        'altendorfer_part_2': {\n",
    "            'file_path': 'Altendorfer_p2_min_1.laz',\n",
    "            'street': 'Altendorfer Straße',\n",
    "            'part': 'part 2',\n",
    "            'subset': 'val',\n",
    "            'short': 'EAD2'\n",
    "        },\n",
    "        'altenessener_part_4': {\n",
    "            'file_path': 'Essen3_p2_min_1.laz',\n",
    "            'street': 'Altenessener Straße',\n",
    "            'part': 'part 4',\n",
    "            'subset': 'val',\n",
    "            'short': 'EAE4'\n",
    "        },\n",
    "        'altenessener_part_5': {\n",
    "            'file_path': 'Essen3_p3_min_1.laz',\n",
    "            'street': 'Altenessener Straße',\n",
    "            'part': 'part 5',\n",
    "            'subset': 'test',\n",
    "            'short': 'EAE5'\n",
    "        }\n",
    "    },\n",
    "    'Hamburg': {\n",
    "        'armgart_straße_part_1': {\n",
    "            'file_path': '000274_v2_min_1.laz',\n",
    "            'street': 'Armgartstraße',\n",
    "            'part': 'part 1',\n",
    "            'subset': 'val',\n",
    "            'short': 'HAG1'\n",
    "        },\n",
    "        'armgart_straße_part_2': {\n",
    "            'file_path': '000275_000276_min_1.laz',\n",
    "            'street': 'Armgartstraße',\n",
    "            'part': 'part 2',\n",
    "            'subset': 'test',\n",
    "            'short': 'HAG2'\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain tree instance segmentation for each point cloud file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_variants = [\n",
    "    (\"w_c\", \"watershed_crown_top_positions\", False),\n",
    "    (\"w_tc\", \"watershed_matched_tree_positions\", False),\n",
    "    (\"w_v\", \"watershed_matched_tree_positions\", True),\n",
    "    (\"w_rg\", \"full\", True),\n",
    "]\n",
    "\n",
    "for dataset in datasets:\n",
    "    os.makedirs(os.path.join(base_dir, 'Data', dataset), exist_ok=True)\n",
    "    for file_id, file_infos in datasets[dataset].items():\n",
    "        for algorithm_variant, algorithm_name, correct_watershed in algorithm_variants:\n",
    "            for (experiment_label, classification_column) in [\n",
    "                ('gt', 'classification_target'),\n",
    "                ('dl', 'classification_prediction')\n",
    "            ]:\n",
    "                if experiment_label == \"gt\" and dataset == \"TreeML\":\n",
    "                    continue\n",
    "\n",
    "                print(\"Process\", file_id, algorithm_variant, experiment_label)\n",
    "                \n",
    "                instance_seg_folder = os.path.join(base_dir, 'Data', dataset, '5_instance_segmentation')\n",
    "                os.makedirs(instance_seg_folder, exist_ok=True)\n",
    "                visualization_folder = os.path.join(base_dir, 'Data', dataset, '6_instance_segmentation_visualizations',\n",
    "                                                    file_id, experiment_label)\n",
    "                os.makedirs(visualization_folder, exist_ok=True)\n",
    "                metrics_folder = os.path.join(base_dir, 'Metrics', dataset, file_id, algorithm_variant)\n",
    "                os.makedirs(metrics_folder, exist_ok=True)\n",
    "\n",
    "                read_columns = [\"x\", \"y\", \"z\", \"instance_id\", \"classification_target\", \"classification_prediction\"]\n",
    "                point_cloud = read(os.path.join(base_dir, 'Data', dataset, '3_semantic_segmentation_processed',\n",
    "                                                file_infos[\"file_path\"]), columns=read_columns)\n",
    "\n",
    "                algorithm = MultiStageAlgorithm(trunk_class_id=1, crown_class_id=2, branch_class_id=3,\n",
    "                                                algorithm=algorithm_name,\n",
    "                                                correct_watershed=correct_watershed,\n",
    "                                                downsampling_voxel_size=0.03,\n",
    "                                                visualization_folder=visualization_folder)\n",
    "\n",
    "                point_cloud[\"instance_id_predicted\"] = algorithm(point_cloud,\n",
    "                                                                 f'{dataset}_{file_id}_{algorithm_variant}_{experiment_label}',\n",
    "                                                                 semantic_segmentation_column=classification_column)\n",
    "\n",
    "                file_name = f'{\".\".join(file_infos[\"file_path\"].split(\".\")[:-1])}_{algorithm_variant}_{experiment_label}.laz'\n",
    "                columns_to_keep = [\"x\", \"y\", \"z\", \"instance_id\", \"instance_id_predicted\", \"classification_target\", \"classification_prediction\"]\n",
    "                point_cloud.to(os.path.join(instance_seg_folder, file_name), columns=columns_to_keep)\n",
    "\n",
    "                runtime_stats = algorithm.runtime_stats()\n",
    "                runtime_stats[\"Dataset\"] = dataset\n",
    "                runtime_stats[\"FileID\"] = file_id\n",
    "                runtime_stats[\"Street\"] = file_infos['street']\n",
    "                runtime_stats[\"Part\"] = file_infos['part']\n",
    "                runtime_stats[\"SemanticSegmentation\"] = experiment_label\n",
    "                file_name = f'{\".\".join(file_infos[\"file_path\"].split(\".\")[:-1])}_instance_segmentation_{algorithm_variant}_{experiment_label}_runtime.csv'\n",
    "                runtime_stats.to_csv(os.path.join(metrics_folder, file_name), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate tree instance segmentation metrics for each point cloud file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_variants = [\n",
    "    (\"w_c\", \"watershed_crown_top_positions\", False),\n",
    "    (\"w_tc\", \"watershed_matched_tree_positions\", False),\n",
    "    (\"w_v\", \"watershed_matched_tree_positions\", True),\n",
    "    (\"w_rg\", \"full\", True),\n",
    "]\n",
    "\n",
    "for dataset in datasets:\n",
    "    os.makedirs(os.path.join(base_dir, 'Data', dataset), exist_ok=True)\n",
    "    for file_id, file_infos in datasets[dataset].items():\n",
    "        for algorithm_variant, algorithm_name, correct_watershed in algorithm_variants:\n",
    "            for (experiment_label, classification_column) in [\n",
    "                ('gt', 'classification_target'),\n",
    "                ('dl', 'classification_prediction')\n",
    "            ]:\n",
    "                if experiment_label == \"gt\" and dataset == \"TreeML\":\n",
    "                    continue\n",
    "\n",
    "                print(\"Process\", file_id, algorithm_variant, experiment_label)\n",
    "                \n",
    "                instance_seg_folder = os.path.join(base_dir, 'Data', dataset, '5_instance_segmentation')\n",
    "                metrics_folder = os.path.join(base_dir, 'Metrics', dataset, file_id, algorithm_variant)\n",
    "                os.makedirs(metrics_folder, exist_ok=True)\n",
    "                \n",
    "                file_name = f'{\".\".join(file_infos[\"file_path\"].split(\".\")[:-1])}_{algorithm_variant}_{experiment_label}.laz'\n",
    "                point_cloud = read(os.path.join(instance_seg_folder, file_name))\n",
    "                point_cloud = point_cloud[np.logical_or(point_cloud[\"classification_target\"].isin([1, 2, 3]),\n",
    "                                                        point_cloud[\"classification_prediction\"].isin([1, 2, 3]))]\n",
    "\n",
    "                matched_target_ids, matched_predicted_ids, instance_metrics = match_instances(point_cloud[\"instance_id\"].to_numpy(),\n",
    "                                                                                  point_cloud[\"instance_id_predicted\"].to_numpy())\n",
    "\n",
    "                metrics = instance_segmentation_metrics(matched_target_ids, matched_predicted_ids, instance_metrics)\n",
    "\n",
    "                metrics[\"Dataset\"] = dataset\n",
    "                metrics[\"SemanticSegmentation\"] = experiment_label.upper()\n",
    "                metrics[\"FileID\"] = file_id\n",
    "                metrics[\"Street\"] = file_infos['street']\n",
    "                metrics[\"Part\"] = file_infos['part']\n",
    "                metrics[\"Short\"] = file_infos['short']\n",
    "\n",
    "                metrics = pd.DataFrame([metrics])\n",
    "            \n",
    "                file_name = \".\".join(file_infos['file_path'].split(\".\")[:-1]) + f\"_instance_segmentation_{algorithm_variant}_{experiment_label}.csv\"\n",
    "                metrics.to_csv(os.path.join(metrics_folder, file_name), index=False)\n",
    "\n",
    "                file_name = \".\".join(file_infos['file_path'].split(\".\")[:-1]) + f\"_instance_segmentation_ious_{algorithm_variant}_{experiment_label}.csv\"\n",
    "                instance_metrics[\"SemanticSegmentation\"] = experiment_label.upper()\n",
    "                instance_metrics.to_csv(os.path.join(metrics_folder, file_name), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Aggregate metrics for all point cloud files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_camel_case(snake_str):\n",
    "    return \"\".join(x.capitalize() for x in snake_str.lower().split(\"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_decimals = 5\n",
    "output_decimnals_runtime = 2\n",
    "\n",
    "metrics = []\n",
    "runtime_results = []\n",
    "\n",
    "add_runtime_description = True\n",
    "for algorithm_variant in [\"w_rg\", \"w_c\", \"w_tc\", \"w_v\"]:\n",
    "    for dataset in datasets:\n",
    "        dataset_metrics = []\n",
    "        dataset_instance_metrics = []\n",
    "        for file_id, file_infos in datasets[dataset].items():\n",
    "            for experiment_label in [\"gt\", \"dl\"]:\n",
    "                metrics_folder = os.path.join(base_dir, 'Metrics', dataset, file_id, algorithm_variant)\n",
    "\n",
    "                file_name = f'{\".\".join(file_infos[\"file_path\"].split(\".\")[:-1])}_instance_segmentation_{algorithm_variant}_{experiment_label}_runtime.csv'\n",
    "                runtime_metrics_file = os.path.join(metrics_folder, file_name)\n",
    "                if os.path.exists(runtime_metrics_file) and algorithm_variant == \"w_rg\":\n",
    "                    runtime_metrics = pd.read_csv(runtime_metrics_file)\n",
    "                    aggregated_metrics = [{\n",
    "                        \"Description\": \"Detection of trunk positions\",\n",
    "                        \"Runtime\": runtime_metrics[runtime_metrics[\"Description\"].isin([\"Clustering of trunk points\", \"Filtering of trunk clusters\", \"Computation of trunk positions\"])][\"Runtime\"].sum()\n",
    "                    },\n",
    "                    {\n",
    "                        \"Description\": \"Construction of canopy height model\",\n",
    "                        \"Runtime\": runtime_metrics[runtime_metrics[\"Description\"].isin([\"Height map computation\"])][\"Runtime\"].sum()\n",
    "                    },\n",
    "                    {\n",
    "                        \"Description\": \"Region growing segmentation\",\n",
    "                        \"Runtime\": runtime_metrics[runtime_metrics[\"Description\"].isin([\"Seed Mask computation\", \"Computation of crown distance fields\", \"Region growing\"])][\"Runtime\"].sum()\n",
    "                    },\n",
    "                    {\n",
    "                        \"Description\": \"Correction of Watershed segmentation\",\n",
    "                        \"Runtime\": runtime_metrics[runtime_metrics[\"Description\"].isin([\"Voronoi correction\"])][\"Runtime\"].sum()\n",
    "                    }\n",
    "                    ]\n",
    "                    runtime_metrics = runtime_metrics[runtime_metrics[\"Description\"].isin([\"Voxel-based downsampling\", \"Detection of crown top positions\", \"Position matching\", \"Watershed segmentation\", \"Upsampling of labels\", \"Total\"])]\n",
    "                    runtime_metrics = pd.concat([runtime_metrics, pd.DataFrame(aggregated_metrics)])\n",
    "                    runtime_column = to_camel_case(f\"{dataset}_{file_id}_{algorithm_variant}_{experiment_label}\")\n",
    "                    runtime_column = runtime_column.replace(\"1\", \"One\").replace(\"2\", \"Two\")\n",
    "                    runtime_metrics[runtime_column] = runtime_metrics[\"Runtime\"]\n",
    "                    if add_runtime_description:\n",
    "                        runtime_results.append(runtime_metrics[\"Description\"])\n",
    "                        add_runtime_description = False\n",
    "                    runtime_results.append(runtime_metrics[runtime_column])\n",
    "\n",
    "                file_name = \".\".join(file_infos['file_path'].split(\".\")[:-1]) + f\"_instance_segmentation_{algorithm_variant}_{experiment_label}.csv\"\n",
    "                instance_segmentation_metrics_file = os.path.join(metrics_folder, file_name)\n",
    "\n",
    "                if os.path.exists(instance_segmentation_metrics_file):\n",
    "                    current_metrics = pd.read_csv(instance_segmentation_metrics_file)\n",
    "                    current_metrics[\"Algorithm\"] = algorithm_variant.replace(\"_\", \"-\").upper()\n",
    "                    metrics.append(current_metrics)\n",
    "                    dataset_metrics.append(current_metrics)\n",
    "\n",
    "                file_name = \".\".join(file_infos['file_path'].split(\".\")[:-1]) + f\"_instance_segmentation_ious_{algorithm_variant}_{experiment_label}.csv\"\n",
    "                instance_metrics_file = os.path.join(metrics_folder, file_name)\n",
    "\n",
    "                if os.path.exists(instance_metrics_file):\n",
    "                    current_metrics = pd.read_csv(instance_metrics_file)\n",
    "                    dataset_instance_metrics.append(current_metrics)\n",
    "\n",
    "        if len(dataset_metrics) > 0:\n",
    "            dataset_metrics = pd.concat(dataset_metrics)\n",
    "            dataset_instance_metrics = pd.concat(dataset_instance_metrics)\n",
    "            for experiment_label in [\"GT\", \"DL\"]:\n",
    "                if experiment_label == \"GT\" and dataset == \"TreeML\":\n",
    "                    continue\n",
    "                total_tp = dataset_metrics[dataset_metrics[\"SemanticSegmentation\"] == experiment_label][\"tp\"].sum()\n",
    "                total_fp = dataset_metrics[dataset_metrics[\"SemanticSegmentation\"] == experiment_label][\"fp\"].sum()\n",
    "                total_fn = dataset_metrics[dataset_metrics[\"SemanticSegmentation\"] == experiment_label][\"fn\"].sum()\n",
    "                total_f1_score = 2 * total_tp / (2 * total_tp + total_fp + total_fn)\n",
    "                total_m_iou = dataset_instance_metrics[dataset_instance_metrics[\"SemanticSegmentation\"] == experiment_label][\"IoU\"].mean()\n",
    "                total_m_precision = dataset_instance_metrics[dataset_instance_metrics[\"SemanticSegmentation\"] == experiment_label][\"Precision\"].mean()\n",
    "                total_m_recall = dataset_instance_metrics[dataset_instance_metrics[\"SemanticSegmentation\"] == experiment_label][\"Recall\"].mean()\n",
    "                metrics.append(pd.DataFrame([{\n",
    "                    \"Dataset\": dataset,\n",
    "                    \"FileID\": \"Total\",\n",
    "                    \"Street\": \"Total\",\n",
    "                    \"Part\": \"\",\n",
    "                    \"Short\": f\"{dataset[0].upper()}Total\",\n",
    "                    \"Algorithm\": algorithm_variant.replace(\"_\", \"-\").upper(),\n",
    "                    \"SemanticSegmentation\": experiment_label,\n",
    "                    \"panoptic_quality\": total_f1_score * total_m_iou,\n",
    "                    \"tp\": total_tp,\n",
    "                    \"fp\": total_fp,\n",
    "                    \"fn\": total_fn,\n",
    "                    \"detection_f1_score\": total_f1_score,\n",
    "                    \"detection_precision\": total_tp / (total_tp + total_fp),\n",
    "                    \"detection_recall\": total_tp / (total_tp + total_fn),\n",
    "                    \"segmentation_m_iou\": total_m_iou,\n",
    "                    \"segmentation_m_precision\": total_m_precision,\n",
    "                    \"segmentation_m_recall\": total_m_recall,\n",
    "                }]))\n",
    "\n",
    "if len(runtime_results) > 0:\n",
    "    runtime_results = pd.concat(runtime_results, axis=1)\n",
    "    for column in runtime_results.columns:\n",
    "        if column != \"Description\":\n",
    "            runtime_results[column] = np.round(runtime_results[column].to_numpy(), output_decimnals_runtime)\n",
    "    sorted_runtime_metrics = [\"Voxel-based downsampling\", \"Detection of trunk positions\", \"Construction of canopy height model\", \"Detection of crown top positions\", \"Position matching\", \"Watershed segmentation\", \"Correction of Watershed segmentation\", \"Region growing segmentation\", \"Upsampling of labels\", \"Total\"]\n",
    "    runtime_results[\"Description\"] = runtime_results[\"Description\"].astype(\"category\")\n",
    "    runtime_results[\"Description\"] = runtime_results[\"Description\"].cat.set_categories(sorted_runtime_metrics)\n",
    "    runtime_results = runtime_results.sort_values([\"Description\"])\n",
    "    runtime_results.to_csv(os.path.join(base_dir, 'Metrics', 'runtime.csv'), index=False)\n",
    "\n",
    "if len(metrics) > 0:\n",
    "    metrics_df = pd.concat(metrics)\n",
    "\n",
    "    ablation_pq = []\n",
    "    for file_id in metrics_df[\"FileID\"].unique():\n",
    "        if file_id == \"Total\":\n",
    "            continue\n",
    "        for semantic_segmentation in [\"GT\", \"DL\"]:\n",
    "            current_metrics = {}\n",
    "            current_metrics[\"Dataset\"] = metrics_df[metrics_df[\"FileID\"] == file_id][\"Dataset\"].to_list()[0]\n",
    "            current_metrics[\"Street\"] = metrics_df[metrics_df[\"FileID\"] == file_id][\"Street\"].to_list()[0]\n",
    "            current_metrics[\"Part\"] = metrics_df[metrics_df[\"FileID\"] == file_id][\"Part\"].to_list()[0]\n",
    "            current_metrics[\"SemanticSegmentation\"] = semantic_segmentation\n",
    "            for algorithm_variant in [\"W-C\", \"W-TC\", \"W-V\", \"W-RG\"]:\n",
    "                metric = metrics_df[(metrics_df[\"FileID\"] == file_id) & (metrics_df[\"SemanticSegmentation\"] == semantic_segmentation) & (metrics_df[\"Algorithm\"] == algorithm_variant)]\n",
    "                if len(metric) > 0:\n",
    "                    assert len(metric) == 1\n",
    "                    current_metrics[algorithm_variant.replace(\"-\", \"\")] = np.round(metric[\"panoptic_quality\"].to_list()[0], output_decimals)\n",
    "            ablation_pq.append(current_metrics)\n",
    "    ablation_pq = pd.DataFrame(ablation_pq)\n",
    "    ablation_pq.to_csv(os.path.join(base_dir, 'Metrics', \"instance_segmentation_ablation_panoptic_quality.csv\"), index=False)\n",
    "\n",
    "    metric_columns = [\"tp\", \"fp\", \"fn\", \"panoptic_quality\", \"detection_f1_score\", \"detection_precision\", \"detection_recall\", \"segmentation_m_iou\", \"segmentation_m_precision\", \"segmentation_m_recall\"]\n",
    "    renamed_metric_columnns = [\"DetectionFScore\"]\n",
    "    for metric_column in metric_columns:\n",
    "        metrics_df[metric_column] = np.round(metrics_df[metric_column].to_numpy(), output_decimals)\n",
    "        if metric_column != \"detection_f1_score\":\n",
    "            renamed_metric_columnns.append(to_camel_case(metric_column))\n",
    "    metrics_df = metrics_df.rename({column: to_camel_case(column) for column in metric_columns}, axis=1)\n",
    "    metrics_df = metrics_df.rename({\"DetectionF1Score\": \"DetectionFScore\"}, axis=1)\n",
    "    metrics_df = metrics_df[[\"Dataset\", \"Street\", \"Part\", \"Short\", \"Algorithm\", \"SemanticSegmentation\", *renamed_metric_columnns]]\n",
    "    metrics_df.to_csv(os.path.join(base_dir, 'Metrics', \"instance_segmentation_metrics.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = metrics_df[(metrics_df[\"Street\"] == \"Total\") & (metrics_df[\"Algorithm\"]).isin([\"W-RG\"]) & (metrics_df[\"SemanticSegmentation\"]).isin([\"DL\", \"GT\"])]\n",
    "selected = selected[[\"Dataset\", \"Street\", \"Part\", \"Algorithm\", \"DetectionRecall\"]]\n",
    "selected.sort_values(by=\"Dataset\", inplace=True)\n",
    "selected"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
