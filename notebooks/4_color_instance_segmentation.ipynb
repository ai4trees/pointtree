{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colorization of Instance Segmentation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pointtree.visualization import color_instance_segmentation\n",
    "from pointtree.io import PointCloudReader, PointCloudWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Point cloud files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '<insert path>'\n",
    "\n",
    "datasets = {\n",
    "    'TreeML': {\n",
    "        # '2023-01-09_tum_campus_0': {\n",
    "        #     'file_path': '2023-01-09_tum_campus_demo_0_trunks.las',\n",
    "        #     'street': '2023-01-09\\_tum\\_campus',\n",
    "        #     'part': '',\n",
    "        #     'subset': 'test',\n",
    "        #     'short': 'TTC'\n",
    "        # },\n",
    "        # '2023-01-09_tum_campus_1': {\n",
    "        #     'file_path': '2023-01-09_tum_campus_demo_1.laz',\n",
    "        #     'street': '2023-01-09\\_tum\\_campus',\n",
    "        #     'part': '',\n",
    "        #     'subset': 'test',\n",
    "        #     'short': 'TTC'\n",
    "        # },\n",
    "        # '2023-01-09_tum_campus_2': {\n",
    "        #     'file_path': '2023-01-09_tum_campus_demo_2.las',\n",
    "        #     'street': '2023-01-09\\_tum\\_campus',\n",
    "        #     'part': '',\n",
    "        #     'subset': 'test',\n",
    "        #     'short': 'TTC'\n",
    "        # },\n",
    "        # '2023-01-09_tum_campus_3': {\n",
    "        #     'file_path': '2023-01-09_tum_campus_demo_3.las',\n",
    "        #     'street': '2023-01-09\\_tum\\_campus',\n",
    "        #     'part': '',\n",
    "        #     'subset': 'test',\n",
    "        #     'short': 'TTC'\n",
    "        # },\n",
    "        # '2023-01-09_tum_campus_4': {\n",
    "        #     'file_path': '2023-01-09_tum_campus_demo_4.las',\n",
    "        #     'street': '2023-01-09\\_tum\\_campus',\n",
    "        #     'part': '',\n",
    "        #     'subset': 'test',\n",
    "        #     'short': 'TTC'\n",
    "        # },\n",
    "        # '2023-01-09_tum_campus_5': {\n",
    "        #     'file_path': '2023-01-09_tum_campus_demo_5.las',\n",
    "        #     'street': '2023-01-09\\_tum\\_campus',\n",
    "        #     'part': '',\n",
    "        #     'subset': 'test',\n",
    "        #     'short': 'TTC'\n",
    "        # },\n",
    "        # '2023-01-09_tum_campus_6': {\n",
    "        #     'file_path': '2023-01-09_tum_campus_demo_6.las',\n",
    "        #     'street': '2023-01-09\\_tum\\_campus',\n",
    "        #     'part': '',\n",
    "        #     'subset': 'test',\n",
    "        #     'short': 'TTC'\n",
    "        # },\n",
    "        # '2023-01-09_tum_campus_7': {\n",
    "        #     'file_path': '2023-01-09_tum_campus_demo_7.laz',\n",
    "        #     'street': '2023-01-09\\_tum\\_campus',\n",
    "        #     'part': '',\n",
    "        #     'subset': 'test',\n",
    "        #     'short': 'TTC'\n",
    "        # },\n",
    "        # '2023-01-09_tum_campus_8': {\n",
    "        #     'file_path': '2023-01-09_tum_campus_demo_8.laz',\n",
    "        #     'street': '2023-01-09\\_tum\\_campus',\n",
    "        #     'part': '',\n",
    "        #     'subset': 'test',\n",
    "        #     'short': 'TTC'\n",
    "        # },\n",
    "        # '2023-01-09_tum_campus_9': {\n",
    "        #     'file_path': '2023-01-09_tum_campus_demo_9.laz',\n",
    "        #     'street': '2023-01-09\\_tum\\_campus',\n",
    "        #     'part': '',\n",
    "        #     'subset': 'test',\n",
    "        #     'short': 'TTC'\n",
    "        # },\n",
    "        '2023-01-09_tum_campus': {\n",
    "            'file_path': '2023-01-09_tum_campus.laz',\n",
    "            'street': '2023-01-09\\_tum\\_campus',\n",
    "            'part': '',\n",
    "            'subset': 'test',\n",
    "            'short': 'TTC'\n",
    "        },\n",
    "        '2023-01-12_57': {\n",
    "            'file_path': '2023-01-12_57.laz',\n",
    "            'street': '2023-01-12\\_57',\n",
    "            'part': '',\n",
    "            'subset': 'test',\n",
    "            'short': 'T12\\_57'\n",
    "        },\n",
    "        '2023-01-12_58': {\n",
    "            'file_path': '2023-01-12_58.laz',\n",
    "            'street': '2023-01-12\\_58',\n",
    "            'part': '',\n",
    "            'subset': 'test',\n",
    "            'short': 'T12\\_58'\n",
    "        },\n",
    "        '2023-01-16_12': {\n",
    "            'file_path': '2023-01-16_12.laz',\n",
    "            'street': '2023-01-16\\_12',\n",
    "            'part': '',\n",
    "            'subset': 'test',\n",
    "            'short': 'T16\\_12'\n",
    "        },\n",
    "        '2023-01-16_43': {\n",
    "            'file_path': '2023-01-16_43.laz',\n",
    "            'street': '2023-01-16\\_43',\n",
    "            'part': '',\n",
    "            'subset': 'test',\n",
    "            'short': 'T16\\_43'\n",
    "        },\n",
    "    },\n",
    "    'Essen': {\n",
    "        'altendorfer_part_1': {\n",
    "            'file_path': 'Altendorfer_p1_min_1.laz',\n",
    "            'street': 'Altendorfer Straße',\n",
    "            'part': 'part 1',\n",
    "            'subset': 'val',\n",
    "            'short': 'EAD1'\n",
    "        },\n",
    "        'altendorfer_part_2': {\n",
    "            'file_path': 'Altendorfer_p2_min_1.laz',\n",
    "            'street': 'Altendorfer Straße',\n",
    "            'part': 'part 2',\n",
    "            'subset': 'val',\n",
    "            'short': 'EAD2'\n",
    "        },\n",
    "        'altenessener_part_4': {\n",
    "            'file_path': 'Essen3_p2_min_1.laz',\n",
    "            'street': 'Altenessener Straße',\n",
    "            'part': 'part 4',\n",
    "            'subset': 'val',\n",
    "            'short': 'EAE4'\n",
    "        },\n",
    "        'altenessener_part_5': {\n",
    "            'file_path': 'Essen3_p3_min_1.laz',\n",
    "            'street': 'Altenessener Straße',\n",
    "            'part': 'part 5',\n",
    "            'subset': 'test',\n",
    "            'short': 'EAE5'\n",
    "        }\n",
    "    },\n",
    "    'Hamburg': {\n",
    "        'armgart_straße_part_1': {\n",
    "            'file_path': '000274_v2_min_1.laz',\n",
    "            'street': 'Armgartstraße',\n",
    "            'part': 'part 1',\n",
    "            'subset': 'val',\n",
    "            'short': 'HAG1'\n",
    "        },\n",
    "        'armgart_straße_part_2': {\n",
    "            'file_path': '000275_000276_min_1.laz',\n",
    "            'street': 'Armgartstraße',\n",
    "            'part': 'part 2',\n",
    "            'subset': 'test',\n",
    "            'short': 'HAG2'\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Color point clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PointCloudReader()\n",
    "writer = PointCloudWriter()\n",
    "\n",
    "for dataset in datasets:\n",
    "    for file_id, file_infos in datasets[dataset].items():\n",
    "        for algorithm_variant in [\"w_c\", \"w_tc\", \"w_v\", \"w_rg\"]:\n",
    "            for experiment_label in [\"gt\", \"dl\"]:\n",
    "\n",
    "                file_name = f'{\".\".join(file_infos[\"file_path\"].split(\".\")[:-1])}_{algorithm_variant}_{experiment_label}.' + ('las' if file_infos[\"file_path\"].endswith(\"las\") else \"laz\")\n",
    "                file_path = os.path.join(base_dir, 'Data', dataset, '5_instance_segmentation', file_name)\n",
    "                if os.path.exists(file_path):\n",
    "                    print(\"Process\", file_id)\n",
    "                    point_cloud_io_data = reader.read(file_path)\n",
    "                    point_cloud = point_cloud_io_data.data\n",
    "                    # point_cloud = filter_outliers(point_cloud)\n",
    "\n",
    "                    point_cloud = color_instance_segmentation(point_cloud, instance_id_column=\"instance_id_predicted\")\n",
    "\n",
    "                    colored_dir = os.path.join(base_dir, 'Data', dataset, '7_instance_segmentation_colored')\n",
    "                    os.makedirs(colored_dir, exist_ok=True)\n",
    "                \n",
    "                    point_cloud_io_data.data = point_cloud\n",
    "                    writer.write(point_cloud_io_data, os.path.join(colored_dir, file_name), columns=[\"x\", \"y\", \"z\", \"r\", \"g\", \"b\"])\n",
    "\n",
    "                    if experiment_label == \"dl\":\n",
    "                        metrics_file_name = \".\".join(file_infos['file_path'].split(\".\")[:-1]) + f\"_instance_segmentation_ious_{algorithm_variant}_{experiment_label}.csv\"\n",
    "                        metrics_folder = os.path.join(base_dir, 'Metrics', dataset, file_id, algorithm_variant)\n",
    "                        instance_metrics = pd.read_csv(os.path.join(metrics_folder, metrics_file_name))\n",
    "\n",
    "                        matched_target_ids = instance_metrics[\"Target\"]\n",
    "                        matched_predicted_ids = instance_metrics[\"Prediction\"]\n",
    "                        fp_ids = np.setdiff1d(np.unique(point_cloud[\"instance_id_predicted\"]), matched_predicted_ids)\n",
    "                        fn_ids = np.setdiff1d(np.unique(point_cloud[\"instance_id\"]), matched_target_ids)\n",
    "\n",
    "                        point_cloud.loc[point_cloud[\"classification_target\"] == 0, \"instance_id\"] = -1\n",
    "                        point_cloud.loc[point_cloud[\"classification_target\"] == 4, \"instance_id\"] = -1\n",
    "                        point_cloud = color_instance_segmentation(point_cloud,\n",
    "                                                                  instance_id_column=\"instance_id_predicted\",\n",
    "                                                                  target_instance_id_column=\"instance_id\",\n",
    "                                                                  fn_ids=fn_ids)\n",
    "                    \n",
    "                        point_cloud_io_data.data = point_cloud\n",
    "                        file_name = f'{\".\".join(file_infos[\"file_path\"].split(\".\")[:-1])}_{algorithm_variant}_{experiment_label}_fn.laz'\n",
    "                        writer.write(point_cloud_io_data, os.path.join(colored_dir, file_name), columns=[\"x\", \"y\", \"z\", \"r\", \"g\", \"b\"])\n",
    "\n",
    "                        point_cloud = color_instance_segmentation(point_cloud,\n",
    "                                                                  instance_id_column=\"instance_id_predicted\",\n",
    "                                                                  target_instance_id_column=\"instance_id\",\n",
    "                                                                  fp_ids=fp_ids)\n",
    "                    \n",
    "                        point_cloud_io_data.data = point_cloud\n",
    "                        file_name = f'{\".\".join(file_infos[\"file_path\"].split(\".\")[:-1])}_{algorithm_variant}_{experiment_label}_fp.laz'\n",
    "                        writer.write(point_cloud_io_data, os.path.join(colored_dir, file_name), columns=[\"x\", \"y\", \"z\", \"r\", \"g\", \"b\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba_kdtree import KDTree\n",
    "import numpy\n",
    "\n",
    "def filter_outliers(point_cloud):\n",
    "    non_tree_mask = point_cloud[\"instance_id\"] == -1\n",
    "    non_tree_points = point_cloud[[\"x\", \"y\", \"z\"]].to_numpy()[non_tree_mask]\n",
    "    kd_tree = KDTree(non_tree_points)\n",
    "    dists, _, _ = kd_tree.query(non_tree_points, k=2)\n",
    "\n",
    "    dists = dists[:, 1]\n",
    "    outlier_mask = dists > 0.08\n",
    "\n",
    "    overall_mask = numpy.ones(len(point_cloud), dtype=bool)\n",
    "    overall_mask[non_tree_mask][outlier_mask] = False\n",
    "\n",
    "    return point_cloud[overall_mask].copy()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
